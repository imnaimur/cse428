{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zj9QK-5eYuUj"
      },
      "source": [
        "#Assignment 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIKMxRxdV36S"
      },
      "source": [
        "Max collaborators = 3\n",
        "\n",
        "Hand holding is low for this assignment. Adjust accordingly. In case of confusion, feel free to reach out to your lab faculties. Good luck."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "MDCLdmpUD1e-"
      },
      "outputs": [],
      "source": [
        "COLLABORATORS_NAME = \"Naimur Rahman,Md. Mukaddimul Kabir, Aritra Chakraborty\"\n",
        "COLLABORATORS_ID = \"24241138, 22141006, 22101892\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUBbKxIOEHxW"
      },
      "source": [
        "#Loading dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "_XqDjPoJEHQz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import color,io, img_as_float\n",
        "from sklearn.model_selection import train_test_split,GridSearchCV\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import torch\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#add if more dependencies are required here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIxBvO6NpzQM",
        "outputId": "98565987-a0ad-48d0-acf6-99e51b17aa4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "\n",
        "device = \"cuda\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1spTeOkGPY7w"
      },
      "source": [
        "#Loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "Qr-Otwz8EvRH"
      },
      "outputs": [],
      "source": [
        "id =  24241138 #insert id of any one collaborator in int\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "x_train, y_train = sklearn.utils.resample(x_train, y_train, replace = False, n_samples = 5000, random_state = id, stratify = y_train)\n",
        "x_test, y_test = sklearn.utils.resample(x_test, y_test, replace = False, n_samples = 1000, random_state = id, stratify = y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq6eQJ8bEAZ7"
      },
      "source": [
        "#Task 1: Training a logistic regressor [10 Marks]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTManISXPccm"
      },
      "source": [
        "###1. Convert the images to grayscale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "k5f2Cf_VIeq6"
      },
      "outputs": [],
      "source": [
        "def rgbToGray(array):\n",
        "    gray = []\n",
        "    for img in array:\n",
        "        gray.append(color.rgb2gray(img))\n",
        "    gray = np.array(gray)\n",
        "    return gray\n",
        "x_train_gray = rgbToGray(x_train)\n",
        "x_test_gray = rgbToGray(x_test)\n",
        "x_train_gray_cnn = rgbToGray(x_train)\n",
        "x_test_gray_cnn = rgbToGray(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhUfuuFrPm1g"
      },
      "source": [
        "### 2. Prepare the grayscale images for logistic regressor (reshape and normalize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vft4Fy_wYTXm"
      },
      "source": [
        "Use z-score normalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "NEDttQUgPxBJ"
      },
      "outputs": [],
      "source": [
        "x_train_gray = x_train_gray.reshape(x_train_gray.shape[0],-1)\n",
        "x_test_gray = x_test_gray.reshape(x_test_gray.shape[0],-1)\n",
        "\n",
        "x_train = x_train_gray.astype(np.float32)\n",
        "x_test = x_test_gray.astype(np.float32)\n",
        "\n",
        "x_train = (x_train - x_train.mean(axis = 0, keepdims = True))/(x_train.std(axis = 0, keepdims = True) + 1e-4)\n",
        "x_test = (x_test - x_test.mean(axis = 0, keepdims = True))/(x_test.std(axis = 0, keepdims = True) + 1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpcARCUFPxef"
      },
      "source": [
        "###3. Create a validation set (20%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "xGYGUTJLQY8N"
      },
      "outputs": [],
      "source": [
        "\n",
        "x_train,x_val,y_train,y_val = train_test_split(x_train,y_train,test_size=0.2, random_state=42,stratify=y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG8fGagsTXaM"
      },
      "source": [
        "###4. Compute class weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyiQ4upFTzAI"
      },
      "source": [
        "Go through this link - https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "zDq5JrXtTx4n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "796a8c2a-69cc-49b3-c061-8dff0107c734"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "{np.uint8(0): np.float64(1.0), np.uint8(1): np.float64(1.0), np.uint8(2): np.float64(1.0), np.uint8(3): np.float64(1.0), np.uint8(4): np.float64(1.0), np.uint8(5): np.float64(1.0), np.uint8(6): np.float64(1.0), np.uint8(7): np.float64(1.0), np.uint8(8): np.float64(1.0), np.uint8(9): np.float64(1.0)}\n"
          ]
        }
      ],
      "source": [
        "class_weight_comp = compute_class_weight(\n",
        "    class_weight=\"balanced\",\n",
        "    classes=np.unique(y_train),\n",
        "      y=y_train.flatten())\n",
        "print(class_weight_comp)\n",
        "class_weight_comp = dict(zip(np.unique(y_train),class_weight_comp))\n",
        "print(class_weight_comp)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS1CvHnfQZui"
      },
      "source": [
        "###5. Run the logistic regressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KD1xXEEwQgga"
      },
      "source": [
        "Use L2 regularizer. Find the C hyperparameter value through grid search. Pick the testing values according to your understanding. Use at least 3 test values and at max 5. Use the computed class weights while training your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFd-FUO1pzQP",
        "outputId": "caaa54a9-487c-4d18-fde0-25e4048abe42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train.shape:  (4000, 1024)\n",
            "y_train.shape:  (4000, 1)\n",
            "y_test.shape (1000, 1)\n"
          ]
        }
      ],
      "source": [
        "print(\"x_train.shape: \",x_train.shape)\n",
        "print(\"y_train.shape: \",y_train.shape)\n",
        "print(\"y_test.shape\",y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "F0HmASjZpzQP"
      },
      "outputs": [],
      "source": [
        "from sklearn.exceptions import ConvergenceWarning\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "q4zfIE3FQf9U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c78625b-cab2-4c0d-8592-db8f729d9324"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression(C=0.01, class_weight='balanced', max_iter=1000)\n",
            "{'C': 0.01, 'class_weight': 'balanced', 'max_iter': 1000, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
            "0.24225190480528677\n"
          ]
        }
      ],
      "source": [
        "model = LogisticRegression()\n",
        "\n",
        "params_grid = {\n",
        "    \"C\": [0.01, 0.02, 0.05, 0.08, 0.1],\n",
        "    \"penalty\": ['l2'],\n",
        "    \"solver\": ['lbfgs'],\n",
        "    \"max_iter\": [1000, 2000, 3000],\n",
        "    \"class_weight\": ['balanced']\n",
        "}\n",
        "\n",
        "\n",
        "logistic_regressor = GridSearchCV(\n",
        "    model,\n",
        "    params_grid,\n",
        "    cv=3,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=4\n",
        ")\n",
        "\n",
        "logistic_regressor.fit(x_train,y_train.ravel())\n",
        "\n",
        "print(logistic_regressor.best_estimator_)\n",
        "print(logistic_regressor.best_params_)\n",
        "print(logistic_regressor.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyUigal5RQi7"
      },
      "source": [
        "###6. Evaluate the logistic regressor on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9A0MkrypzQQ",
        "outputId": "9c81e13c-21ad-40f4-d05c-2d02e9411af0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.262\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_pred = logistic_regressor.predict(x_test)\n",
        "test_acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1COtQ3Yo5XX"
      },
      "source": [
        "###7. Write code to pick up a random image from the test set and display it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "L8HM5Asuo99j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "29301412-a62c-40fb-fcfb-f4714b546fca"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "random_index = np.random.randint(y_test.shape[0], size = 1)\n",
        "\n",
        "\n",
        "img = x_test[random_index].reshape(32, 32)\n",
        "\n",
        "plt.imshow(img)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kLNUZtzpC3U"
      },
      "source": [
        "###8. Print the predicted class vs the original class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJb3nHZgo4ZQ"
      },
      "source": [
        "Do not print out the numerical class. Map it from here - https://keras.io/2/api/datasets/cifar10/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "hjvYQALUpD3n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d2967d3-7f99-439f-de68-4410b0c37f02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 1: Predicted = automobile, Original = automobile\n",
            "Sample 2: Predicted = ship, Original = truck\n",
            "Sample 3: Predicted = automobile, Original = automobile\n",
            "Sample 4: Predicted = bird, Original = ship\n",
            "Sample 5: Predicted = automobile, Original = automobile\n",
            "Sample 6: Predicted = deer, Original = dog\n",
            "Sample 7: Predicted = truck, Original = truck\n",
            "Sample 8: Predicted = truck, Original = automobile\n",
            "Sample 9: Predicted = frog, Original = deer\n",
            "Sample 10: Predicted = airplane, Original = airplane\n",
            "Sample 11: Predicted = frog, Original = frog\n",
            "Sample 12: Predicted = ship, Original = airplane\n",
            "Sample 13: Predicted = truck, Original = ship\n",
            "Sample 14: Predicted = horse, Original = cat\n",
            "Sample 15: Predicted = ship, Original = deer\n",
            "Sample 16: Predicted = airplane, Original = dog\n",
            "Sample 17: Predicted = frog, Original = cat\n",
            "Sample 18: Predicted = automobile, Original = truck\n",
            "Sample 19: Predicted = frog, Original = cat\n",
            "Sample 20: Predicted = dog, Original = airplane\n"
          ]
        }
      ],
      "source": [
        "y_pred = logistic_regressor.predict(x_test)\n",
        "y_test_c = y_test.ravel()\n",
        "class_names = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
        "               \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
        "\n",
        "# Print predicted vs original class for first 20 test samples\n",
        "for i in range(20):\n",
        "    print(f\"Sample {i+1}: Predicted = {class_names[y_pred[i]]}, Original = {class_names[y_test_c[i]]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xd6ctJcyRa5L"
      },
      "source": [
        "#Task 2: Training a convolutional neural network [20 Marks]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqvJrLWmRuyL"
      },
      "source": [
        "We will not strictly control how you implement this code. You can use either Tensorflow or PyTorch. However the structure of the network must be -\n",
        "\n",
        "```\n",
        "Input -> Conv1 -> Conv2 -> Conv3 -> Fully Connected 1 -> Fully Connected 2 -> Output\n",
        "```\n",
        "\n",
        "Use activation functions and pooling as you want. Feel free to adjust dimensions as you need. Set the hyperparameters yourself. Use a maximum learning rate of 0.01 and a maximum epoch number of 100. Use AdamW as optimizer.\n",
        "\n",
        "**Try achieving good accuracy. There are marks for that.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg4e1kUBUCTq"
      },
      "source": [
        "###1. Train the model on grayscale images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgoBiFcwUF1b"
      },
      "source": [
        "You do not need to use the validation set. Train on the initial train set. Recalculate the class weights again and pass it to the optimizer function. Normalize the images before processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFL7lGbApzQR"
      },
      "source": [
        "#Imports for task 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "9s6Gh8f_pzQR"
      },
      "outputs": [],
      "source": [
        "\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "zF7-vXC1RpRX"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, c_in=1, output_dim=128):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(c_in, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, output_dim, kernel_size=3, padding=1)\n",
        "        self.pool  = nn.MaxPool2d(2,2)\n",
        "        self.fc1   = nn.Linear(output_dim*4*4, output_dim)\n",
        "        self.fc2   = nn.Linear(output_dim, 64)\n",
        "        self.out   = nn.Linear(64, 10)\n",
        "        self.relu  = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = self.pool(self.relu(self.conv3(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.out(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQveLxP3wh9L",
        "outputId": "b3b12e42-60c4-419b-8447-f954b0668973"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "\n",
        "x_train = x_train.astype(np.float32)/255.0\n",
        "x_test  = x_test.astype(np.float32)/255.0\n",
        "\n",
        "y_train = y_train.astype(np.int64).squeeze()\n",
        "y_test  = y_test.astype(np.int64).squeeze()\n",
        "\n",
        "\n",
        "x_train_gray = np.array([color.rgb2gray(img) for img in x_train], dtype=np.float32)\n",
        "x_test_gray  = np.array([color.rgb2gray(img) for img in x_test], dtype=np.float32)\n",
        "\n",
        "\n",
        "mean_train = x_train_gray.mean(axis=(0,1,2), keepdims=True)\n",
        "std_train  = x_train_gray.std(axis=(0,1,2), keepdims=True) + 1e-4\n",
        "\n",
        "x_train_gray = (x_train_gray - mean_train)/std_train\n",
        "x_test_gray  = (x_test_gray - mean_train)/std_train"
      ],
      "metadata": {
        "id": "7s13MqC9wtwr"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_tensor = torch.tensor(x_train_gray, device=device).unsqueeze(1)\n",
        "x_test_tensor  = torch.tensor(x_test_gray, device=device).unsqueeze(1)\n",
        "y_train_tensor = torch.tensor(y_train, device=device)\n",
        "y_test_tensor  = torch.tensor(y_test, device=device)"
      ],
      "metadata": {
        "id": "wHjexPJg0ZIB"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mtExjH0pzQk",
        "outputId": "a2e19386-6b36-4bc5-b599-2c042989e3f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/120 | Loss: 1.6342 | Accuracy: 0.4062\n",
            "Epoch 10/120 | Loss: 0.4372 | Accuracy: 0.8493\n",
            "Epoch 20/120 | Loss: 0.1599 | Accuracy: 0.9433\n",
            "Epoch 30/120 | Loss: 0.0740 | Accuracy: 0.9744\n",
            "Epoch 40/120 | Loss: 0.0548 | Accuracy: 0.9811\n",
            "Epoch 50/120 | Loss: 0.0442 | Accuracy: 0.9852\n",
            "Epoch 60/120 | Loss: 0.0384 | Accuracy: 0.9872\n",
            "Epoch 70/120 | Loss: 0.0311 | Accuracy: 0.9894\n",
            "Epoch 80/120 | Loss: 0.0227 | Accuracy: 0.9920\n",
            "Epoch 90/120 | Loss: 0.0292 | Accuracy: 0.9904\n",
            "Epoch 100/120 | Loss: 0.0276 | Accuracy: 0.9909\n",
            "Epoch 110/120 | Loss: 0.0290 | Accuracy: 0.9899\n",
            "Epoch 120/120 | Loss: 0.0164 | Accuracy: 0.9946\n"
          ]
        }
      ],
      "source": [
        "batch_size = 128\n",
        "learning_rate = 0.001\n",
        "epochs = 120\n",
        "\n",
        "classifier = CNN(c_in=1).to(device)\n",
        "optimizer = torch.optim.AdamW(classifier.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "train_size = x_train_tensor.shape[0]\n",
        "batch_len = int(np.ceil(train_size / batch_size))\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    classifier.train()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "\n",
        "    for i in range(batch_len):\n",
        "        start = i * batch_size\n",
        "        end   = min((i+1)*batch_size, train_size)\n",
        "\n",
        "        x_batch = x_train_tensor[start:end]\n",
        "        y_batch = y_train_tensor[start:end]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = classifier(x_batch)\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * x_batch.size(0)\n",
        "        total_correct += (y_pred.argmax(dim=1) == y_batch).sum().item()\n",
        "\n",
        "    epoch_loss = total_loss / train_size\n",
        "    epoch_acc  = total_correct / train_size\n",
        "\n",
        "    if (epoch+1) % 10 == 0 or epoch == 0:\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {epoch_loss:.4f} | Accuracy: {epoch_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH9S4fLiUMza"
      },
      "source": [
        "###2. Evaluate the model on the grayscale test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "g7Jwzg6EUQ2x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28579f48-f467-460d-dd32-5b51ddbb0e8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7098999619483948\n"
          ]
        }
      ],
      "source": [
        "classifier.eval()\n",
        "with torch.no_grad():\n",
        "    y_test_pred = classifier(x_test_tensor)\n",
        "    test_acc = (y_test_pred.argmax(dim=1) == y_test_tensor).float().mean().item()\n",
        "print(\"Test Accuracy:\", test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Wvna5JKokP8"
      },
      "source": [
        "###3. Write code to pick up a random image from the test set and display it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "BIqJEI77olBD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "7991ae09-0e04-46a3-95e9-90cefeb4a91d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAFItJREFUeJzt3EmPXWfVBeC3XH3jqrJjx73LiTGGJEiREhAIBkwQMyQkfiT/ATFgwBCR4AQS27LjptxW5+r7y2xPv3chrsgnPc94a9epe5qlMzhrZDAYDBoAtNbO/K8PAIDvD6EAQBEKABShAEARCgAUoQBAEQoAFKEAQBnrHfz9738/tIMYGRmJ5kdHR7tnz5zJcm9iYmJou5PjPj09jXan3yCmv3ki+V3S4z45ORna7vQ3OT4+7p49OjqKdh8cHHTPJtdsa63Nz893z05NTUW7k+v28PAw2r2/vx/NJ/uT37u17Nynkuvw3bt30e6//vWv/+eMNwUAilAAoAgFAIpQAKAIBQCKUACgCAUAilAAoAgFAIpQAKAIBQBKd/dR2oHyfekQSjtKkmNJO2fS/3OYko6aYZ6fpMuotaxDKO0bSo8l+Q3THqZE2tmUXLeTk5PR7uR+S7uPUsn5TK/xsbHuR+dQO7WG8Uz5/jylAPifEwoAFKEAQBEKABShAEARCgAUoQBAEQoAFKEAQBEKAJT+b7VD4+PjQ5ltLasXSKsLks/G0+Me5qfxw5RWNCTz6f+Z7E5rLtL5pI7g4OBgaLuT66q14VZuJMe9v78f7d7Z2UkPp1t6LyfXyt7eXrQ7mR/GufSmAEARCgAUoQBAEQoAFKEAQBEKABShAEARCgAUoQBAEQoAFKEAQOkuTUk7hJJ+omFKu3VGR0e7Z5OepHR3etzT09NDO5a0t+fw8HBouxPp+Unnh3mNJ+cnNczent3d3aHMtpY/g5J7Ir1/tre3u2eH+Rumx93DmwIARSgAUIQCAEUoAFCEAgBFKABQhAIARSgAUIQCAEUoAFC6ay6Sz7pba21srHt1NNtaVgEwNTUV7R4fHx/a7uS4JyYmot0zMzPR/DAlNRep5PxMTk5Gu9Oai/39/aHtTq6V5DdJ7ezsRPNbW1vds2n9wzDv5ePj42h3UrkxzDqU9NnZw5sCAEUoAFCEAgBFKABQhAIARSgAUIQCAEUoAFCEAgBFKABQhAIApbs4I+liaS3revk+9cJMT093z6bdOkm/StqXkvYNJfuTjp/WWjs4OIjmE0n/TdqVMxgMovm1tbXu2fQ3Se6J9LiTe2J3dzfavbm52T07zGs2lV7jq6urQzqS1ubn57tnZ2dn/+t/35sCAEUoAFCEAgBFKABQhAIARSgAUIQCAEUoAFCEAgBFKABQumsuzp49Gy2emJjonk0/00/qIsbGuv/FWHrcyWf9e3t7Qz2WZD6tF0gqA9KKk2Gez9evX0fzL1++7J5NaxSS+yetOjg5OYnmE0ktxsbGRrQ7fQZtb293zx4dHUW7k/n0uJP55Drp5U0BgCIUAChCAYAiFAAoQgGAIhQAKEIBgCIUAChCAYAiFAAoQgGA0l0ks7CwMLSDSLqMWhtud8vIyEj3bNJl1FrWC7O2thbtTvuJks6U0dHRoR3Lzs5OtDvpJ0r7hh4+fBjNb21tdc+m98/c3Fz3bNoHlfT2pPfmyspK92zaCZQey8zMTPfsuXPnot3J/ZP2e6X323+bNwUAilAAoAgFAIpQAKAIBQCKUACgCAUAilAAoAgFAIpQAKB0fx8/GAyGdhDJZ/etZTUX6XEn1QhptcTm5mb37Pr6erQ7+ey+tdYuXrw4tN1JRcdXX30V7X737l33bHpdpZUb4+Pj3bNpjUKy+9WrV9HupG5lmBU0yf/YWl5b8sEHH3TPXrt2LdqdVFckdSjpfHp+enhTAKAIBQCKUACgCAUAilAAoAgFAIpQAKAIBQCKUACgCAUAilAAoHR3H6W9I4mkR6S1rM8o6XlprbW9vb2hzKbz6e89OjoazSfSfpWnT592zz5//jzanZz7pIfnP5H0Gd24cSPafffu3e7ZJ0+eRLuXl5e7Z7e3t6PdIyMjQ9ud9F61NtxrJZlP782xse7Hcvzs7Nr5X98IwP9bQgGAIhQAKEIBgCIUAChCAYAiFAAoQgGAIhQAKEIBgCIUACjdJRuHh4dDO4iJiYloPjmWtC9la2urezbtBEp6SsbHx6Pdp6en0XzSO5P+n6urq92z6blPOmeSHp7Wsq6c1rJOm9nZ2Wj3zZs3u2fTfq/k/PzkJz+Jdn/66afds/fv349237t3L5pP7v30WpmamuqenZycjHZPT093z6b3fQ9vCgAUoQBAEQoAFKEAQBEKABShAEARCgAUoQBAEQoAFKEAQOmuuUg/p06qEdIKjeSz/qTOIT2W5FP31rLP19Pqgr29vWg+qZdIjyU590lVRGvDrQpJ6zw2Nja6Z//+979Huy9cuNA9e3R0FO1eWlrqnv3DH/4Q7f7kk0+6Z//0pz9Fu1dWVqL5Fy9edM9evnw52p1ct2mFxtzcXPds+nzr4U0BgCIUAChCAYAiFAAoQgGAIhQAKEIBgCIUAChCAYAiFAAoQgGA0t19lHTOtNbayclJ9+zBwUG0O+knGmbvyPz8fLQ7+U2Wl5ej3Wl/VNJllfarJLvHxrovwXg+7VVK+qBaa+3q1avds2k/0c7OTvfs5ORktPvWrVvds+l9n/QNJd1RrbW2vr4ezV+5cqV79vr169Hut2/fds/u7+9Hu5PzORgMot09vCkAUIQCAEUoAFCEAgBFKABQhAIARSgAUIQCAEUoAFCEAgAl6xhIFgd1BMP4VPs/OY7Wss/6x8fHo93JZ/1bW1vR7rTSYWVlpXs2PT/JsSTVH61l5zOtaEjqH1pr7Xe/+1337Ndffx3tfv78effs3t5etHuYFQ0fffTR0HYnFTSttXbx4sXu2bTKJbmX3717F+1OfpfZ2dlodw9vCgAUoQBAEQoAFKEAQBEKABShAEARCgAUoQBAEQoAFKEAQBEKAJTuIpm0R2ZycrJ7Nt2dzKf9Kol09+bmZvds2mmS9sIcHR0NZba1rBNqZ2cn2j3MbqrFxcVoPukcevr0abQ76b66evVqtHtpaal7dmFhIdr9zTffdM++fv062p32E927d6979vj4ONp9enraPft9ek708KYAQBEKABShAEARCgAUoQBAEQoAFKEAQBEKABShAEARCgCU7pqLy5cvR4uTaoTkk/7WWjs8POyeTT5HT+3u7kbzybGklQtpzcX09HT37MHBQbQ7+Ux/amoq2p3UXKT1Kelvvry83D2bVGK01toPf/jD7tk7d+5Eu8+fP989u76+Hu1O7omzZ89Gu1P//Oc/u2dHR0ej3cm1lZ77kZGR7tn0/unhTQGAIhQAKEIBgCIUAChCAYAiFAAoQgGAIhQAKEIBgCIUAChCAYDS3X30i1/8Ilr87t277tnnz59Hu58+fdo9m/b2JB0oq6ur0e6ks2mYnUCtZf03g8Eg2p10t8zOzka7x8a6L9l2fHwc7Z6YmIjmk3P0q1/9Ktr9wQcfdM+Oj49Hu5NrJb0OFxYWhnIcrbX25ZdfRvMnJyfds8m92Vp2TyQ9cK21Njk5OZTZXt4UAChCAYAiFAAoQgGAIhQAKEIBgCIUAChCAYAiFAAoQgGAIhQAKN1FMj/4wQ+ixRsbG92zMzMz0e6kc2hnZyfanfTlpJ0mSW9P2guTHkvyf16+fDnanfRNpf1EyfyVK1ei3deuXYvmk16gTz/9NNp99+7d7tm1tbVo9/LycjSf2N7e7p598uRJtHt+fj6a/9GPftQ9m/aYJd1uSRdYa1kv2eLiYrS7hzcFAIpQAKAIBQCKUACgCAUAilAAoAgFAIpQAKAIBQCKUACgdPcufPvtt9HipF7i4sWL0e4bN250z759+zba/fz58+7ZtEJjdnY2mk8k1RKtZRUNaf1DUnWQ2t/f755Nr6tf//rX0fyLFy+6ZweDQbT7/fff7549OTmJdr969ap7Nq2gefnyZffso0ePot0///nPo/nkWnn48GG0O3muvH79Otr98ccfd89euHAh2t3DmwIARSgAUIQCAEUoAFCEAgBFKABQhAIARSgAUIQCAEUoAFCEAgClu/voiy++iBYnvUAfffRRtHtycrJ7Nu3hSXph0i6j8+fPd89ubW1Fu3d3d6P5c+fOdc+mvT1J50za3ZL0GaXnJ+35uX37dvds2k2VdI2lu9fW1rpnNzY2ot3Jfb+0tBTt/tnPfhbNj411P97a48ePo9337t3rnv3zn/8c7V5dXe2enZ+fj3b38KYAQBEKABShAEARCgAUoQBAEQoAFKEAQBEKABShAEARCgCU7u/ADw8Po8Wjo6Pds69fv452J1UUb968iXbPzc11zyY1B61ldR4rKyvR7gcPHkTzy8vL3bPHx8fR7qRyI61o+M1vftM9m9ShtJbVP7TW2vj4ePdscj+01tqXX37ZPTsyMhLt/uabb4a2O7l/0uvq0aNH0XxS5XJ0dBTtPj09jeYTX331Vfdsen56eFMAoAgFAIpQAKAIBQCKUACgCAUAilAAoAgFAIpQAKAIBQCKUACgdHcfpc6fP989u7OzE+3e2Njonr1x40a0+9KlS92zS0tL0e6kiyftykn6oFrLfsObN29Gu2/dutU9+9Of/jTa/dvf/rZ79h//+Ee0++nTp9F80juTdjy9fPmye/bx48fR7u+++657Nu0ESubTfq/5+flofnFxsXs26etqLeu9Snvjpqenu2cHg0G0u4c3BQCKUACgCAUAilAAoAgFAIpQAKAIBQCKUACgCAUAilAAoHTXXMzNzUWLNzc3hzLbWmtnzvRnWVJb0VpWz5F+dp9UV8zMzES7x8ayxpKkouOXv/zl0HZ/+OGH0e7nz593z37xxRfR7rTmIrG/vx/NJ9UIy8vL0e5nz551z6Y1F0lFQ1pBs7CwEM0nv3laubG+vt49e3JyEu1OfsP0uHt4UwCgCAUAilAAoAgFAIpQAKAIBQCKUACgCAUAilAAoAgFAIpQAKB0F+YkfRytZd0ti4uL0e7d3d3u2Tdv3kS7NzY2umcHg0G0O+l6efnyZbT7woUL0fxnn33WPbu6uhrtfvToUffsH//4x2j33/72t+7ZFy9eRLuvXbsWzScdXFNTU9Hus2fPds+enp5Gu5MOoU8++STa/f7773fPJl1greX3cnIdpvdy8jxMe6+S3devX4929/CmAEARCgAUoQBAEQoAFKEAQBEKABShAEARCgAUoQBAEQoAlO6ai48//jha/PDhw+7Zzc3NaPeVK1e6Z1+9ehXtPjk56Z5NKjFaa+3t27fds+kn/en5efbsWffsX/7yl2j31tZW9+zYWPcl2FrLPuu/c+dOtDupaGgtq2dJKzSSa3x5eTna/eDBg+7Z2dnZaHdy7tP6lMnJyWh+Zmame/bWrVvR7vPnz3fPbm9vR7vHx8e7Z9Pz08ObAgBFKABQhAIARSgAUIQCAEUoAFCEAgBFKABQhAIARSgAUIQCAKW7eCbt2JiYmOieXVhYiHbPzc11z+7v70e7k36VtFfpyZMn3bMXL16Mdo+MjETzSffRzZs3o91Xr17tnp2amop23759u3s26Q9qLeuzaS3rYTp79my0O+m/Sa6r1lrb2dnpnj0+Po52J8+J5H9srbXT09NoPnmupL1KybEfHh5Gu5NnVvKc7eVNAYAiFAAoQgGAIhQAKEIBgCIUAChCAYAiFAAoQgGAIhQAKEIBgNLdffT48eNocdKZcu7cuWj33t5e9+ydO3ei3Um/yv3796PdSafJmTNZXj98+HBox/L5559Hu+/evds9e3JyEu1OrpX0unr79m00v76+3j2bdh8lXVZJl1F6LJcuXYp2b29vd89ubm5Gu9Pzk/yG6flJOp7evHkT7T44OOienZmZiXb38KYAQBEKABShAEARCgAUoQBAEQoAFKEAQBEKABShAEARCgCU7pqLjY2NaHHySfpgMIh2JxUQKysr0e5vv/22e/bZs2fR7qTSYXR0NNp9/fr1aD45n2kVxdHRUffs4eFhtDupF0j961//iuaTeokrV65Eu5P6gqTOobXWLly40D2b3vePHj3qnk2uk9Zam5iYiOYXFxe7Z9P7LXlmpddsUkGTVK308qYAQBEKABShAEARCgAUoQBAEQoAFKEAQBEKABShAEARCgAUoQBA6e4+mp+fjxbv7u52z6b9NwsLC92zSYdMa619+OGH3bNTU1PR7qRH5vPPP492//jHP47mk96ZpGuqtdbevXvXPZv2Xj148KB7Nu2FmZycjObPnTvXPZv02aTS87O2ttY9e3p6Gu1OerLSTq30/0w6odJ+ouR3SZ6FrWXP2rTbrYc3BQCKUACgCAUAilAAoAgFAIpQAKAIBQCKUACgCAUAilAAoHTXXKSVDrdu3eqeTT67b6217e3t7tkLFy5Eu+/cudM9+/Tp02h3ctzXr1+Pdr948SKaT+olkjqH1lp77733umc3Nzej3ffv3++effPmTbR7aWkpml9cXOyeHR0djXYfHx93z6bnZ2ys+7aPK2iSWpmkrqa1vIri6tWr3bNJJUZrrX333Xfds+k1nvyfZ8+ejXb38KYAQBEKABShAEARCgAUoQBAEQoAFKEAQBEKABShAEARCgAUoQBA6S5BmZiYiBYn82kvTDKfdprs7+93z56cnES7k56SpCeptdZWVlai+aOjo+7ZycnJaPcwf8PPPvuse3Zvby/anXbUJJ1DaYfQy5cvu2dPT0+j3ePj492zOzs70e7p6enu2aSDqbW83ys59uSabS3ra0t745L5ubm5aHcPbwoAFKEAQBEKABShAEARCgAUoQBAEQoAFKEAQBEKABShAEDp/s780qVL0eKkGiGtokg+X3/z5k20+9GjR92zu7u70e4bN250z87MzES7r1+/Hs0nNRdpxcnx8XH37O3bt6PdyW/49ddfR7tTly9f7p5N6yKS2pL19fVod1KjkJzL1vJqkcTbt2+j+VevXnXPJueytdbee++97tm0ziO5N9OKoB7eFAAoQgGAIhQAKEIBgCIUAChCAYAiFAAoQgGAIhQAKEIBgCIUACgjg8Fg8L8+CAC+H7wpAFCEAgBFKABQhAIARSgAUIQCAEUoAFCEAgBFKABQ/g18LwNVh8boGgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "idx = random.randint(0, x_test_tensor.shape[0]-1)\n",
        "img = x_test_tensor[idx].cpu().squeeze()\n",
        "plt.imshow(img, cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HRX0XOwoode"
      },
      "source": [
        "###4. Print the predicted class vs the original class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "L2Ihb9opoq0G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b9e326e-785d-4be9-b812-8efaacedb58b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: cat | True: frog\n"
          ]
        }
      ],
      "source": [
        "class_names = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
        "               \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
        "\n",
        "pred_class = class_names[y_test_pred.argmax(dim=1)[idx]]\n",
        "true_class = class_names[y_test_tensor[idx]]\n",
        "print(f\"Predicted: {pred_class} | True: {true_class}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvO11goFURbC"
      },
      "source": [
        "###5. Train the model on the RGB images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vGs-tpwUpOc"
      },
      "source": [
        "Keep the model same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "fb3Cbv_tUlkK"
      },
      "outputs": [],
      "source": [
        "x_train_tensor = torch.tensor(x_train, dtype=torch.float32, device=device).permute(0,3,1,2)\n",
        "x_test_tensor  = torch.tensor(x_test, dtype=torch.float32, device=device).permute(0,3,1,2)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long, device=device)\n",
        "y_test_tensor  = torch.tensor(y_test, dtype=torch.long, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "epochs = 120\n",
        "\n",
        "classifier = CNN(c_in=3).to(device)\n",
        "optimizer = torch.optim.AdamW(classifier.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "train_size = x_train_tensor.shape[0]\n",
        "batch_len = int(np.ceil(train_size / batch_size))\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    classifier.train()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "\n",
        "    for i in range(batch_len):\n",
        "        start = i * batch_size\n",
        "        end   = min((i+1)*batch_size, train_size)\n",
        "\n",
        "        x_batch = x_train_tensor[start:end]\n",
        "        y_batch = y_train_tensor[start:end]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = classifier(x_batch)\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * x_batch.size(0)\n",
        "        total_correct += (y_pred.argmax(dim=1) == y_batch).sum().item()\n",
        "\n",
        "    epoch_loss = total_loss / train_size\n",
        "    epoch_acc  = total_correct / train_size\n",
        "\n",
        "    if (epoch+1) % 10 == 0 or epoch == 0:\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {epoch_loss:.4f} | Accuracy: {epoch_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocke3Q_p2o4k",
        "outputId": "2ed8c415-f6e8-4c27-b4f1-98fd7c4471e9"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/120 | Loss: 1.5819 | Accuracy: 0.4152\n",
            "Epoch 10/120 | Loss: 0.5586 | Accuracy: 0.8015\n",
            "Epoch 20/120 | Loss: 0.3764 | Accuracy: 0.8669\n",
            "Epoch 30/120 | Loss: 0.2824 | Accuracy: 0.9003\n",
            "Epoch 40/120 | Loss: 0.2276 | Accuracy: 0.9199\n",
            "Epoch 50/120 | Loss: 0.1926 | Accuracy: 0.9315\n",
            "Epoch 60/120 | Loss: 0.1758 | Accuracy: 0.9381\n",
            "Epoch 70/120 | Loss: 0.1608 | Accuracy: 0.9446\n",
            "Epoch 80/120 | Loss: 0.1395 | Accuracy: 0.9512\n",
            "Epoch 90/120 | Loss: 0.1346 | Accuracy: 0.9532\n",
            "Epoch 100/120 | Loss: 0.1264 | Accuracy: 0.9561\n",
            "Epoch 110/120 | Loss: 0.1234 | Accuracy: 0.9564\n",
            "Epoch 120/120 | Loss: 0.1265 | Accuracy: 0.9573\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9DM5SjbUmMl"
      },
      "source": [
        "###6. Evaluate the model on the RGB images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "rHlOvimGUoxe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbb846e9-71b0-42f2-8c8e-a848999f0919"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.6872000098228455\n"
          ]
        }
      ],
      "source": [
        "classifier.eval()\n",
        "with torch.no_grad():\n",
        "    y_test_pred = classifier(x_test_tensor)\n",
        "    test_acc = (y_test_pred.argmax(dim=1) == y_test_tensor).float().mean().item()\n",
        "print(\"Test Accuracy:\", test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXUSCL9GWHcx"
      },
      "source": [
        "###7. Write code to pick up a random image from the test set and display it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "X1rplFeGWTyi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "3c24ea35-fa40-47d6-9bcc-dcf83c38ca09"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "idx = random.randint(0, x_test.shape[0]-1)\n",
        "img = x_test[idx].squeeze()\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEC37E2CWSsp"
      },
      "source": [
        "###8. Print the predicted class vs the original class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "xkSJ2IvDWjKh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "000c7d52-9dff-4333-c8e3-713f63c91706"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: dog | True: dog\n"
          ]
        }
      ],
      "source": [
        "class_names = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
        "               \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
        "\n",
        "pred_class = class_names[y_test_pred.argmax(dim=1)[idx]]\n",
        "true_class = class_names[y_test_tensor[idx]]\n",
        "print(f\"Predicted: {pred_class} | True: {true_class}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pk-B9vNdpG6a"
      },
      "source": [
        "###9. Add one more conv layer and check if it increases the performance for the RGB images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "k6RDr59DpvCY"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, c_in=3, output_dim=128):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(c_in, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(128, output_dim, kernel_size=3, padding=1)\n",
        "        self.pool  = nn.MaxPool2d(2,2)\n",
        "        self.fc1   = nn.Linear(output_dim*2*2, output_dim)  # after 4 pools: 32->16->8->4->2\n",
        "        self.fc2   = nn.Linear(output_dim, 64)\n",
        "        self.out   = nn.Linear(64, 10)\n",
        "        self.relu  = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))  # 32x32 -> 16x16\n",
        "        x = self.pool(self.relu(self.conv2(x)))  # 16x16 -> 8x8\n",
        "        x = self.pool(self.relu(self.conv3(x)))  # 8x8 -> 4x4\n",
        "        x = self.pool(self.relu(self.conv4(x)))  # 4x4 -> 2x2\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.out(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = CNN(c_in=3).to(device)\n",
        "\n",
        "\n",
        "batch_size = 128\n",
        "learning_rate = 0.002\n",
        "epochs = 100\n",
        "\n",
        "optimizer = torch.optim.AdamW(classifier.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "train_size = x_train_tensor.shape[0]\n",
        "batch_len = int(np.ceil(train_size / batch_size))\n",
        "\n",
        "# --- Training loop ---\n",
        "for epoch in range(epochs):\n",
        "    classifier.train()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "\n",
        "    for i in range(batch_len):\n",
        "        start = i * batch_size\n",
        "        end   = min((i+1)*batch_size, train_size)\n",
        "\n",
        "        x_batch = x_train_tensor[start:end]\n",
        "        y_batch = y_train_tensor[start:end]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = classifier(x_batch)\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * x_batch.size(0)\n",
        "        total_correct += (y_pred.argmax(dim=1) == y_batch).sum().item()\n",
        "\n",
        "    epoch_loss = total_loss / train_size\n",
        "    epoch_acc  = total_correct / train_size\n",
        "\n",
        "    if (epoch+1) % 5 == 0 or epoch == 0:\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {epoch_loss:.4f} | Accuracy: {epoch_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPp8utT97PBw",
        "outputId": "5bd946a9-dadc-4ae2-d6d0-c573f7c81047"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 | Loss: 1.6805 | Accuracy: 0.3720\n",
            "Epoch 5/100 | Loss: 0.7616 | Accuracy: 0.7305\n",
            "Epoch 10/100 | Loss: 0.5242 | Accuracy: 0.8155\n",
            "Epoch 15/100 | Loss: 0.3667 | Accuracy: 0.8698\n",
            "Epoch 20/100 | Loss: 0.2867 | Accuracy: 0.8982\n",
            "Epoch 25/100 | Loss: 0.2293 | Accuracy: 0.9170\n",
            "Epoch 30/100 | Loss: 0.1983 | Accuracy: 0.9299\n",
            "Epoch 35/100 | Loss: 0.1686 | Accuracy: 0.9412\n",
            "Epoch 40/100 | Loss: 0.1385 | Accuracy: 0.9521\n",
            "Epoch 45/100 | Loss: 0.1221 | Accuracy: 0.9584\n",
            "Epoch 50/100 | Loss: 0.1206 | Accuracy: 0.9590\n",
            "Epoch 55/100 | Loss: 0.1120 | Accuracy: 0.9616\n",
            "Epoch 60/100 | Loss: 0.0971 | Accuracy: 0.9675\n",
            "Epoch 65/100 | Loss: 0.0917 | Accuracy: 0.9687\n",
            "Epoch 70/100 | Loss: 0.0896 | Accuracy: 0.9698\n",
            "Epoch 75/100 | Loss: 0.0881 | Accuracy: 0.9702\n",
            "Epoch 80/100 | Loss: 0.0861 | Accuracy: 0.9719\n",
            "Epoch 85/100 | Loss: 0.0783 | Accuracy: 0.9739\n",
            "Epoch 90/100 | Loss: 0.0762 | Accuracy: 0.9746\n",
            "Epoch 95/100 | Loss: 0.0703 | Accuracy: 0.9770\n",
            "Epoch 100/100 | Loss: 0.0780 | Accuracy: 0.9754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1TXbZEyUr2a"
      },
      "source": [
        "###10. Use the next markup to write down your observations and reasonings on the performances of the models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTC63sXOU0Uo"
      },
      "source": [
        "The performane of Logistics regression is very poor is has to be.becuse the data structure is complex for it.so can't blame it.but in ***CNN (3 layer)*** we see the accuracy is 99.12 for grayscale and 99.06 for RGB.maybe we could have look for more tuning in batch_size.but when i tried with 120 epocs and lr=0.002 the preformance dropped and in anoter scenario i kept 120 epocs and lr = 0.001 performance increased so i can say learning rate is the factor in this dataset and model.but using 4 layer performaced dropped maybe i made it a bit complex and data got distorted."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}